{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bb544e10-d9e6-4958-af19-367055ea015d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7000, 10)\n",
      "(1500, 10)\n",
      "(1500, 10)\n"
     ]
    }
   ],
   "source": [
    "#Bank User Churn Project\n",
    "#Authors: Flora and Adriana\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import csv\n",
    "\n",
    "Geography = {\"France\": 0.0, \"Spain\": 1.0, \"Germany\": 2.0}\n",
    "Gender = {\"Female\": 0.0, \"Male\": 1}\n",
    "\n",
    "data = [] \n",
    "\n",
    "with open('Churn_Modelling.csv', 'r') as read_obj:\n",
    "    csv_reader = csv.reader(read_obj)\n",
    "    next(csv_reader)  # Skip header row\n",
    "\n",
    "    for row in csv_reader:\n",
    "        row = row[3:]  # Skip first 3 columns\n",
    "        \n",
    "        row[1] = Geography[row[1]]   # Geography\n",
    "        row[2] = Gender[row[2]]      # Gender\n",
    "\n",
    "        data.append(row)\n",
    "\n",
    "# Convert to NumPy array and cast to float32\n",
    "data = np.array(data).astype(np.float32)\n",
    "\n",
    "# Separate features and labels\n",
    "X = data[:, :-1]  # All columns except the last\n",
    "y = data[:, -1]   # Last column is label\n",
    "\n",
    "# Split data\n",
    "X_train, X_t, y_train, y_t = train_test_split(X, y, test_size=0.30, random_state=0)\n",
    "X_validation, X_test, y_validation, y_test = train_test_split(X_t, y_t, test_size=0.5, random_state=0)\n",
    "\n",
    "# Print shapes\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(X_validation.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eb8c13f5-a163-4902-8b93-105d232f9e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gender\n",
      "Male      5457\n",
      "Female    4543\n",
      "Name: count, dtype: int64\n",
      "Users with Credit Score > 650: 5063\n",
      "Users with Credit Score ≤ 650: 4937\n"
     ]
    }
   ],
   "source": [
    "#find biases\n",
    "import pandas as pd\n",
    "\n",
    "# Retrieve gender statistics\n",
    "df = pd.read_csv('Churn_Modelling.csv')\n",
    "print(df['Gender'].value_counts())\n",
    "\n",
    "#Retrive credit score statics\n",
    "above_650 = (df['CreditScore'] > 650).sum()\n",
    "below_or_equal_650 = (df['CreditScore'] <= 650).sum()\n",
    "\n",
    "print(f\"Users with Credit Score > 650: {above_650}\")\n",
    "print(f\"Users with Credit Score ≤ 650: {below_or_equal_650}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2242724c-e720-4ab6-bf25-8e8279f0be63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_validation_scaled = scaler.transform(X_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a925198a-4ec5-46dd-bced-ec12dcf9442b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#knn\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def get_accuracy(y_true, y_predicted):\n",
    "    \"\"\"returns the fraction of correct predictions in y_predicted compared to y_true\"\"\"\n",
    "    counter= 0\n",
    "    for x in range(len(y_true)):\n",
    "        if (y_true[x] == y_predicted[x]):\n",
    "            counter+= 1\n",
    "\n",
    "    accuracy= counter/ len(y_true)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def sklearn_knn_predict(trainX, trainy, testX, distance_metric, k):\n",
    "    model = KNeighborsClassifier(algorithm='brute', n_neighbors= k, metric= distance_metric)\n",
    "    model.fit(trainX, trainy)\n",
    "    predicted = model.predict(testX)\n",
    "\n",
    "    return predicted\n",
    "\n",
    "def knn_grid_search(trainX, trainy, validationX, validationy, distance_metric_list, n_neighbors_list):\n",
    "    \"\"\"For each metric in distance_metric_list \"euclidean, manhattan, etc\", and each value k in n_neighbors_list,\n",
    "    trains knn classifiers with those parameters\n",
    "    on the training data and computes the accuracy on the validation data.\n",
    "    Returns a dictionary mapping each value of the hyperparameter pair (metric, k)\n",
    "    to the accuracy with those hyperparameters on the validation data\n",
    "    \"\"\"\n",
    "    results={}\n",
    "    for k in n_neighbors_list:\n",
    "        \n",
    "        for d in distance_metric_list:\n",
    "            accuracy= get_accuracy(validationy, sklearn_knn_predict(trainX,  trainy, validationX, d, k))\n",
    "            results[d, k] = accuracy\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b07a49b4-d6e2-4f9a-9e6e-7fbeaf9a69b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are metric = euclidean and k = 19 with 0.7873333333333333 accuracy on the validation data\n",
      "Test accuracy: 0.7953333333333333\n"
     ]
    }
   ],
   "source": [
    "#continuation\n",
    "\n",
    "k_values= [1,3,5,7,9,11,13,15,17,19]\n",
    "\n",
    "grid= knn_grid_search(X_train, y_train, X_validation, y_validation, ['euclidean','manhattan'], k_values)\n",
    "\n",
    "highest = max(grid, key=grid.get)\n",
    "\n",
    "\n",
    "metric, k = highest[0], highest[1]\n",
    "validation_accuracy = grid[highest]\n",
    "print('The best parameters are metric =', metric, 'and k =', k, 'with', validation_accuracy, 'accuracy on the validation data')\n",
    "\n",
    "\n",
    "\n",
    "test_accuracy = get_accuracy(y_test, sklearn_knn_predict(X_train, y_train, X_test, metric, k) )\n",
    "print('Test accuracy:', test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3ff92a7a-5e94-400c-a434-04f009963a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0. ... 0. 0. 1.]\n",
      "Validation score on Random Forest:\n",
      "0.854\n",
      "Test score on Random Forest:\n",
      "0.8733333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn import ensemble \n",
    "clf = ensemble.RandomForestClassifier(random_state=0, class_weight='balanced')\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.predict(X_test))\n",
    "\n",
    "print(\"Validation score on Random Forest:\")\n",
    "print(clf.score(X_validation, y_validation))\n",
    "\n",
    "print(\"Test score on Random Forest:\")\n",
    "print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "89ca0829-83d5-4198-890d-eacfed93e8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (10000, 10)\n",
      "Feature importances shape: (10,)\n",
      "Feature names count: 10\n",
      "Feature Importances:\n",
      "Age:0.249691782388163\n",
      "Balance:0.14511338510653754\n",
      "EstimatedSalary:0.13969947380554415\n",
      "CreditScore:0.1355087349020238\n",
      "NumOfProducts:0.1313417475040262\n",
      "Tenure:0.07783857740800411\n",
      "Geography:0.04733175800436427\n",
      "IsActiveMember:0.033496115204982135\n",
      "Gender:0.02157456166202002\n",
      "HasCrCard:0.018403864014334862\n"
     ]
    }
   ],
   "source": [
    "#Random Forest Feature Importance\n",
    "import pandas as pd\n",
    "feature_names = [ 'CreditScore', 'Geography', 'Gender', 'Age', 'Tenure', \n",
    "    'Balance', 'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary'\n",
    "]\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"Feature importances shape:\", clf.feature_importances_.shape)\n",
    "print(\"Feature names count:\", len(feature_names))\n",
    "importances = clf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "sorted_features = [(feature_names[i], importances[i]) for i in indices]\n",
    "\n",
    "print(\"Feature Importances:\")\n",
    "for name, score in sorted_features:\n",
    "    print(\"{name}:{score}\".format(name=name, score=score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b17df034-cbe6-498f-aa18-504c1cbc1fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation score on Logistic Regression:\n",
      "0.6966666666666667\n",
      "Test score on Logistic regression:\n",
      "0.6986666666666667\n"
     ]
    }
   ],
   "source": [
    "#Logistic regression and weight extraction\n",
    "from sklearn import linear_model  \n",
    "logRegres = linear_model.LogisticRegression(random_state=0, class_weight='balanced')\n",
    "logRegres.fit(X_train_scaled, y_train )\n",
    "print(\"Validation score on Logistic Regression:\")\n",
    "print(logRegres.score(X_validation_scaled, y_validation))\n",
    "\n",
    "print(\"Test score on Logistic regression:\")\n",
    "print(logRegres.score(X_test_scaled, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9b170c78-88f7-4fe1-8771-c286507b7b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Weights:\n",
      "[[-0.09302871  0.30914394 -0.28443939  0.8086866  -0.08124474  0.21334037\n",
      "  -0.08732669 -0.01431191 -0.42580662  0.03690607]]\n",
      "P-value for each feature\n",
      "[1.73892188e-003 1.27677977e-040 3.32366810e-019 1.03577361e-128\n",
      " 2.14166317e-002 1.05879654e-024 1.42532740e-007 4.42720255e-001\n",
      " 5.80328653e-033 2.30693167e-001]\n"
     ]
    }
   ],
   "source": [
    "# Get Logistic Regression weights\n",
    "from sklearn.feature_selection import f_regression\n",
    "weights = logRegres.coef_\n",
    "sorted_weights = np.argsort(weights)  # Sort\n",
    "print(\"Feature Weights:\")\n",
    "print(weights)\n",
    "#P-values for each \n",
    "print(\"P-value for each feature\")\n",
    "print(f_regression(X_train_scaled, y_train)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c9ec457b-a8ed-48da-9881-2f40f9080bee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Assuming the data is Non-linear\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracies_val = []\n",
    "accuracies_test = []\n",
    "\n",
    "# for degree in range(1,11):\n",
    "#     poly = PolynomialFeatures(degree=degree)\n",
    "#     model = LogisticRegression(max_iter=20000)\n",
    "#     X_train_poly = poly.fit_transform(X_train)\n",
    "#     X_test_poly = poly.transform(X_test)\n",
    "#     X_validation_poly = poly.transform(X_validation)\n",
    "#     model.fit(X_train_poly, y_train)\n",
    "#     y_val = model.predict(X_validation_poly)\n",
    "#     accuracies_val.append((degree, accuracy_score(y_val, y_validation)))\n",
    "#     y_prediction = model.predict(X_test_poly)\n",
    "#     accuracies_test.append((degree, accuracy_score(y_prediction, y_test)))\n",
    "# print('Accuracies on Validation:' + str(accuracies_val))\n",
    "# print('Accuracies on Validation:' + str(accuracies_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f9d514e0-819d-4bf7-bff8-efd12860d7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score on validation data: 0.5796545105566219\n",
      "F1 Score on testing: 0.62\n"
     ]
    }
   ],
   "source": [
    "#Experiment with f1 score\n",
    "from sklearn.metrics import f1_score\n",
    "#predictions with random forest\n",
    "y_pred_val  = clf.predict(X_validation)\n",
    "y_pred_test  = clf.predict(X_test)\n",
    "#calculate the scores\n",
    "f1_val = f1_score(y_validation, y_pred_val)\n",
    "f1_test = f1_score(y_test, y_pred_test)\n",
    "print(\"F1 Score on validation data:\", f1_val)\n",
    "print(\"F1 Score on testing:\", f1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "77a5e66f-62e1-4b81-9a4f-7c299d06eac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS WAS JUST TO TEST OUR PREDICTIONS INSTEAD OF INPUTTING A WHOLE DATA SET FOR THINGS WE WANT PREDICTED\n",
    "\n",
    "\n",
    "# Age 20\n",
    "F1 = [500, 0, 1, 20, 5, 120000.0, 1, 1, 1, 175000.0]  # High balance, high salary, 1 product\n",
    "F2 = [500, 0, 1, 20, 5, 30000.0, 1, 1, 1, 50000.0]    # Low balance, low salary, 1 product\n",
    "F3 = [500, 0, 1, 20, 5, 120000.0, 4, 1, 1, 175000.0]  # High balance, high salary, 4 products\n",
    "F4 = [500, 0, 1, 20, 5, 30000.0, 4, 1, 1, 50000.0]    # Low balance, low salary, 4 products\n",
    "\n",
    "# Age 45\n",
    "F5 = [500, 0, 1, 45, 5, 120000.0, 1, 1, 1, 175000.0]\n",
    "F6 = [500, 0, 1, 45, 5, 30000.0, 1, 1, 1, 50000.0]\n",
    "F7 = [500, 0, 1, 45, 5, 120000.0, 4, 1, 1, 175000.0]\n",
    "F8 = [500, 0, 1, 45, 5, 30000.0, 4, 1, 1, 50000.0]\n",
    "\n",
    "# Age 70\n",
    "F9 = [500, 0, 1, 70, 5, 120000.0, 1, 1, 1, 175000.0]\n",
    "F10 = [500, 0, 1, 70, 5, 30000.0, 1, 1, 1, 50000.0]\n",
    "F11 = [500, 0, 1, 70, 5, 120000.0, 4, 1, 1, 175000.0]\n",
    "F12 = [500, 0, 1, 70, 5, 30000.0, 4, 1, 1, 50000.0]\n",
    "\n",
    "# Age 95\n",
    "F13 = [500, 0, 1, 95, 5, 120000.0, 1, 1, 1, 175000.0]\n",
    "F14 = [500, 0, 1, 95, 5, 30000.0, 1, 1, 1, 50000.0]\n",
    "F15 = [500, 0, 1, 95, 5, 120000.0, 4, 1, 1, 175000.0]\n",
    "F16 = [500, 0, 1, 95, 5, 30000.0, 4, 1, 1, 50000.0]\n",
    "\n",
    "\n",
    "# # all_features = np.array([F1, F2, F3, F4, F5, F6, F7, F8, F9, F10, F11, F12, F13, F14, F15, F16, F17, F18, F19, F20])\n",
    "\n",
    "# #most likely 1\n",
    "# all_features= np.array([\n",
    "#     [480, 2.0, 1, 30, 1, 150000.0, 3, 0, 0, 180000.0],  # Germany, inactive, high balance, no card\n",
    "#     [500, 2.0, 0, 27, 2, 140000.0, 2, 0, 0, 160000.0],  # Female, high balance, low tenure\n",
    "#     [510, 2.0, 1, 29, 3, 130000.0, 4, 0, 0, 170000.0],  # Many products, inactive\n",
    "#     [450, 1.0, 0, 26, 1, 120000.0, 2, 0, 0, 190000.0],  # Young, low credit score, high salary\n",
    "#     [499, 0.0, 1, 32, 1, 160000.0, 2, 0, 0, 200000.0],  # French male with red flags\n",
    "# ])\n",
    "# #most likely 0\n",
    "# # all_features = np.array([\n",
    "# #     [850, 0.0, 1, 35, 5,     0.0,     1, 1, 1, 70000.0],   # Very high score, no balance\n",
    "# #     [820, 1.0, 0, 45, 7,     0.0,     1, 1, 1, 65000.0],   # Older Spanish woman, low balance\n",
    "# #     [800, 0.0, 1, 40, 6,     1000.0,  1, 1, 1, 90000.0],   # Stable user\n",
    "# #     [780, 1.0, 1, 50, 8,     0.0,     1, 1, 1, 100000.0],  # High salary, active\n",
    "# #     [770, 0.0, 0, 42, 10,    500.0,   1, 1, 1, 80000.0],   # Long tenure, low complexity\n",
    "# #     [760, 1.0, 0, 36, 4,     300.0,   1, 1, 1, 120000.0],  # Simple, active user\n",
    "# #     [750, 0.0, 1, 38, 6,     0.0,     1, 1, 1, 75000.0],   # Clean, stable profile\n",
    "# #     [740, 1.0, 0, 33, 3,     200.0,   1, 1, 1, 68000.0],   # Lower balance, fewer years\n",
    "# # ])\n",
    "\n",
    "# scaled_features = scaler.transform(all_features)\n",
    "# predictions = logRegres.predict(scaled_features)\n",
    "# print(predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ba4b33-c4c4-4d59-b476-2b92f854c216",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bbaa32-3f8a-4015-a731-40c23a21d571",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "204842d3-58c5-47fb-a728-02334e811b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression predictions:\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Random Forest predictions:\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1.\n",
      " 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1.\n",
      " 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "Geography = {\"France\": 0.0, \"Spain\": 1.0, \"Germany\": 2.0}\n",
    "Gender = {\"Female\": 0.0, \"Male\": 1}\n",
    "\n",
    "test_data = []\n",
    "\n",
    "with open('smaller_churn_test_data_alternating_balance.csv', 'r') as read_obj:\n",
    "    csv_reader = csv.reader(read_obj)\n",
    "    next(csv_reader)  # Skip header\n",
    "\n",
    "    for row in csv_reader:\n",
    "        row = row[3:]  # Skip first 3 columns\n",
    "\n",
    "        row[1] = Geography[row[1]]  # Geography\n",
    "        row[2] = Gender[row[2]]     # Gender\n",
    "\n",
    "        test_data.append(row)\n",
    "\n",
    "# Convert to NumPy array and float\n",
    "test_data = np.array(test_data).astype(np.float32)\n",
    "\n",
    "# Apply the same scaler used during training\n",
    "scaled_test_data = scaler.transform(test_data)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_log = logRegres.predict(scaled_test_data)\n",
    "y_pred_rf  = clf.predict(test_data)  # Random Forest doesn't need scaling\n",
    "\n",
    "print(\"Logistic Regression predictions:\\n\", y_pred_log)\n",
    "print(\"Random Forest predictions:\\n\", y_pred_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7448a1db-0e43-4c46-9a33-240b0d8e60ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age 20: Users with different balances, salaries, and product counts\n",
      "[0. 0. 1. 0.]\n",
      "Age 45: Users with different balances, salaries, and product counts\n",
      "[0. 0. 1. 1.]\n",
      "Age 70: Users with different balances, salaries, and product counts\n",
      "[0. 0. 1. 1.]\n",
      "Age 95: Users with different balances, salaries, and product counts\n",
      "[0. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Age 20\n",
    "print(\"Age 20: Users with different balances, salaries, and product counts\")\n",
    "age_20 = np.array([\n",
    "    [500, 0, 1, 20, 5, 120000.0, 1, 1, 1, 175000.0],\n",
    "    [500, 0, 1, 20, 5, 30000.0, 1, 1, 1, 50000.0],\n",
    "    [500, 0, 1, 20, 5, 120000.0, 4, 1, 1, 175000.0],\n",
    "    [500, 0, 1, 20, 5, 30000.0, 4, 1, 1, 50000.0]\n",
    "])\n",
    "print(clf.predict(age_20))\n",
    "\n",
    "# Age 45\n",
    "print(\"Age 45: Users with different balances, salaries, and product counts\")\n",
    "age_45 =  np.array([\n",
    "    [500, 0, 1, 45, 5, 120000.0, 1, 1, 1, 175000.0],\n",
    "    [500, 0, 1, 45, 5, 30000.0, 1, 1, 1, 50000.0],\n",
    "    [500, 0, 1, 45, 5, 120000.0, 4, 1, 1, 175000.0],\n",
    "    [500, 0, 1, 45, 5, 30000.0, 4, 1, 1, 50000.0]\n",
    "])\n",
    "print(clf.predict(age_45))\n",
    "\n",
    "# Age 70\n",
    "print(\"Age 70: Users with different balances, salaries, and product counts\")\n",
    "age_70 =  np.array([\n",
    "    [500, 0, 1, 70, 5, 120000.0, 1, 1, 1, 175000.0],\n",
    "    [500, 0, 1, 70, 5, 30000.0, 1, 1, 1, 50000.0],\n",
    "    [500, 0, 1, 70, 5, 120000.0, 4, 1, 1, 175000.0],\n",
    "    [500, 0, 1, 70, 5, 30000.0, 4, 1, 1, 50000.0]\n",
    "])\n",
    "print(clf.predict(age_70))\n",
    "\n",
    "# Age 95\n",
    "print(\"Age 95: Users with different balances, salaries, and product counts\")\n",
    "age_95 =  np.array([\n",
    "    [500, 0, 1, 95, 5, 120000.0, 1, 1, 1, 175000.0],\n",
    "    [500, 0, 1, 95, 5, 30000.0, 1, 1, 1, 50000.0],\n",
    "    [500, 0, 1, 95, 5, 120000.0, 4, 1, 1, 175000.0],\n",
    "    [500, 0, 1, 95, 5, 30000.0, 4, 1, 1, 50000.0]\n",
    "])\n",
    "print(clf.predict(age_95))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b6e766-c84b-40c1-8cd9-063151a89c61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
